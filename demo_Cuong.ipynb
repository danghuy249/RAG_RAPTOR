{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ce24e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5095275e",
   "metadata": {},
   "source": [
    "### Run demo of owner repo --- done with openAI, but bug with GEMMA2B - but: GEMMA  --------------BUG TÙM LUM VỚI GAMMA2B: KeyError: 'gemma'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912cd8c6-d405-4dfe-8897-46108e6a6af7",
   "metadata": {},
   "source": [
    "# RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631b09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: An OpenAI API key must be set here for application initialization, even if not in use.\n",
    "# If you're not utilizing OpenAI models, assign a placeholder string (e.g., \"not_used\").\n",
    "import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7d995-7beb-40b5-9a44-afd350b7d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cinderella story defined in sample.txt\n",
    "with open('demo/sample.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d51ebd-5597-4fdd-8c37-32636395081b",
   "metadata": {},
   "source": [
    "1) **Building**: RAPTOR recursively embeds, clusters, and summarizes chunks of text to construct a tree with varying levels of summarization from the bottom up. You can create a tree from the text in 'sample.txt' using `RA.add_documents(text)`.\n",
    "\n",
    "2) **Querying**: At inference time, the RAPTOR model retrieves information from this tree, integrating data across lengthy documents at different abstraction levels. You can perform queries on the tree with `RA.answer_question`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f58830-9004-48a4-b50e-61a855511d24",
   "metadata": {},
   "source": [
    "### Building the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3753fcf9-0a8e-4ab3-bf3a-6be38ef6cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raptor import RetrievalAugmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e843edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "RA = RetrievalAugmentation()\n",
    "\n",
    "# construct the tree\n",
    "RA.add_documents(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f219d60a-1f0b-4cee-89eb-2ae026f13e63",
   "metadata": {},
   "source": [
    "### Querying from the tree\n",
    "\n",
    "```python\n",
    "question = # any question\n",
    "RA.answer_question(question)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4037c5-ad5a-424b-80e4-a67b8e00773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How did Cinderella reach her happy ending ?\"\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tree by calling RA.save(\"path/to/save\")\n",
    "SAVE_PATH = \"demo/cinderella\"\n",
    "RA.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e845de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back the tree by passing it into RetrievalAugmentation\n",
    "\n",
    "RA = RetrievalAugmentation(tree=SAVE_PATH)\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc583c48",
   "metadata": {},
   "source": [
    "View Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d647543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raptor import RetrievalAugmentation\n",
    "\n",
    "# Load the saved tree\n",
    "RA = RetrievalAugmentation(tree=\"demo/cinderella\")\n",
    "\n",
    "# Debug helper to inspect the tree structure\n",
    "def inspect_tree(tree):\n",
    "    print(\"Available attributes:\", dir(tree))\n",
    "    print(\"Tree type:\", type(tree))\n",
    "    \n",
    "    # Try to access some common attributes safely\n",
    "    for attr in ['children', 'text', 'content', 'summary', 'data', 'value']:\n",
    "        try:\n",
    "            value = getattr(tree, attr)\n",
    "            print(f\"Attribute '{attr}' exists with type: {type(value)}\")\n",
    "            if isinstance(value, str):\n",
    "                print(f\"First 100 chars: {value[:100]}\")\n",
    "        except AttributeError:\n",
    "            print(f\"Attribute '{attr}' does not exist\")\n",
    "\n",
    "# Inspect the tree\n",
    "tree = RA.tree\n",
    "inspect_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raptor import RetrievalAugmentation\n",
    "\n",
    "def print_full_tree(tree):\n",
    "    def print_layer(layer_num):\n",
    "        print(f\"\\n=== Layer {layer_num} ===\")\n",
    "        if layer_num in tree.layer_to_nodes:\n",
    "            nodes = tree.layer_to_nodes[layer_num]\n",
    "            for i, node in enumerate(nodes, 1):\n",
    "                print(f\"\\nNode {i} in Layer {layer_num}:\")\n",
    "                try:\n",
    "                    # Thử in ra thông tin của node\n",
    "                    print(f\"Node data: {node.__dict__}\")\n",
    "                except:\n",
    "                    print(\"Cannot print node data directly\")\n",
    "\n",
    "    print(\"TREE STRUCTURE:\")\n",
    "    print(\"===============\")\n",
    "    \n",
    "    # In thông tin tổng quan\n",
    "    print(f\"Total layers: {tree.num_layers}\")\n",
    "    print(f\"Total nodes: {len(tree.all_nodes)}\")\n",
    "    print(f\"Leaf nodes: {len(tree.leaf_nodes)}\")\n",
    "    print(f\"Root nodes: {len(tree.root_nodes)}\")\n",
    "    \n",
    "    # In từng layer\n",
    "    for layer in range(tree.num_layers):\n",
    "        print_layer(layer)\n",
    "\n",
    "# Load và in cây\n",
    "RA = RetrievalAugmentation(tree=\"demo/cinderella\")\n",
    "print_full_tree(RA.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6e1d4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Kết quả này cho thấy có một số vấn đề với cấu trúc cây:\n",
    "\n",
    "**Tình trạng hiện tại:**\n",
    "1. `Total layers: 0` - Không có layer nào\n",
    "2. `Total nodes: 10` - Có 10 nodes\n",
    "3. `Leaf nodes: 10` - Có 10 leaf nodes\n",
    "4. `Root nodes: 10` - Có 10 root nodes\n",
    "\n",
    "**Phân tích vấn đề:**\n",
    "1. Đây là một tình trạng bất thường vì:\n",
    "   - Số layer = 0 trong khi config đặt `Num Layers: 5`\n",
    "   - Tất cả nodes (10) đều là cả root và leaf\n",
    "   - Không có cấu trúc phân cấp (hierarchy)\n",
    "\n",
    "2. Có thể có một trong các nguyên nhân sau:\n",
    "   - Dữ liệu input quá nhỏ hoặc không đủ để tạo cấu trúc phân cấp\n",
    "   - Clustering không hoạt động đúng\n",
    "   - Bug trong quá trình xây dựng cây\n",
    "\n",
    "**Đề xuất kiểm tra:**\n",
    "```python\n",
    "from raptor import RetrievalAugmentation\n",
    "\n",
    "def debug_tree_building(tree):\n",
    "    print(\"1. Configuration Check:\")\n",
    "    print(f\"Configured layers: {tree.num_layers}\")\n",
    "    \n",
    "    print(\"\\n2. Node Content Check:\")\n",
    "    for node in tree.all_nodes:\n",
    "        try:\n",
    "            # Thử in thông tin của một vài node để xem nội dung\n",
    "            print(f\"\\nNode ID: {id(node)}\")\n",
    "            print(f\"Node attributes: {dir(node)}\")\n",
    "            # Thử lấy nội dung nếu có\n",
    "            if hasattr(node, 'data'):\n",
    "                print(f\"Data: {node.data[:100]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error inspecting node: {e}\")\n",
    "    \n",
    "    print(\"\\n3. Clustering Check:\")\n",
    "    print(f\"Number of unique nodes: {len(set(tree.all_nodes))}\")\n",
    "    print(f\"Are all nodes root nodes? {all(node in tree.root_nodes for node in tree.all_nodes)}\")\n",
    "    print(f\"Are all nodes leaf nodes? {all(node in tree.leaf_nodes for node in tree.all_nodes)}\")\n",
    "\n",
    "# Load và debug\n",
    "RA = RetrievalAugmentation(tree=\"demo/cinderella\")\n",
    "debug_tree_building(RA.tree)\n",
    "```\n",
    "\n",
    "**Các bước khắc phục:**\n",
    "1. Kiểm tra dữ liệu input của Cinderella\n",
    "2. Thử tăng giá trị threshold hoặc điều chỉnh top_k\n",
    "3. Có thể cần rebuild tree với các tham số khác:\n",
    "```python\n",
    "RA = RetrievalAugmentation()\n",
    "RA.build_tree(\n",
    "    text_chunks,  # your input data\n",
    "    config=dict(\n",
    "        threshold=0.7,  # tăng threshold\n",
    "        top_k=3,       # giảm top_k\n",
    "        num_layers=3   # giảm số layer\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "Bạn có thể chia sẻ thêm về dữ liệu input và cách bạn xây dựng cây không? Điều này sẽ giúp xác định chính xác nguyên nhân của vấn đề."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c32659",
   "metadata": {},
   "source": [
    "build với tham số khác như bạn đề xuất đi. \n",
    "\n",
    "Đọc @raptor  để config chuẩn và code tiếp trong @demo_Cuong.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6077e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raptor import RetrievalAugmentationConfig, ClusterTreeConfig\n",
    "\n",
    "# Cấu hình cho Tree Builder\n",
    "tree_builder_config = ClusterTreeConfig(\n",
    "    max_tokens=150,          # Tăng max tokens cho mỗi node\n",
    "    num_layers=3,           # Giảm số layer\n",
    "    threshold=0.7,          # Tăng threshold cho clustering\n",
    "    top_k=3,               # Giảm top_k\n",
    "    reduction_dimension=5,  # Giảm dimension cho clustering\n",
    "    clustering_params={\n",
    "        \"max_length_in_cluster\": 2000,  # Giới hạn độ dài text trong cluster\n",
    "        \"threshold\": 0.2,              # Threshold cho clustering\n",
    "        \"verbose\": True                # In log chi tiết\n",
    "    }\n",
    ")\n",
    "\n",
    "# Cấu hình tổng thể\n",
    "config = RetrievalAugmentationConfig(\n",
    "    tree_builder_config=tree_builder_config,\n",
    "    tree_builder_type=\"cluster\",\n",
    "    # Cấu hình cho retriever\n",
    "    tr_threshold=0.6,\n",
    "    tr_top_k=3,\n",
    "    tr_selection_mode=\"top_k\"\n",
    ")\n",
    "\n",
    "# Khởi tạo RAPTOR với config mới\n",
    "RA = RetrievalAugmentation(config=config)\n",
    "\n",
    "# Load và xử lý text\n",
    "with open('demo/sample.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Xây dựng cây\n",
    "RA.add_documents(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346bc164",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Hãy phân tích kết quả xây dựng cây:\n",
    "\n",
    "1. **Cấu hình ban đầu đã được khởi tạo thành công** với các tham số:\n",
    "\n",
    "```17:29:raptor/cluster_tree_builder.py\n",
    "class ClusterTreeConfig(TreeBuilderConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        reduction_dimension=10,\n",
    "        clustering_algorithm=RAPTOR_Clustering,  # Default to RAPTOR clustering\n",
    "        clustering_params={},  # Pass additional params as a dict\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reduction_dimension = reduction_dimension\n",
    "        self.clustering_algorithm = clustering_algorithm\n",
    "        self.clustering_params = clustering_params\n",
    "```\n",
    "\n",
    "\n",
    "2. **Quá trình xây dựng cây:**\n",
    "\n",
    "- Tạo được 7 leaf nodes (nodes lá) ban đầu\n",
    "- Bắt đầu xây dựng Layer 0:\n",
    "  - Tạo được 4 clusters/nodes mới với độ dài text khác nhau:\n",
    "    - Node 1: 151 tokens -> 100 tokens sau summarize\n",
    "    - Node 2: 283 tokens -> 100 tokens sau summarize  \n",
    "    - Node 3: 425 tokens -> 100 tokens sau summarize\n",
    "    - Node 4: 47 tokens -> 95 tokens sau summarize\n",
    "\n",
    "- Dừng ở Layer 1 vì không đủ nodes để tạo layer tiếp theo (theo điều kiện):\n",
    "\n",
    "```95:100:raptor/cluster_tree_builder.py\n",
    "            if len(node_list_current_layer) <= self.reduction_dimension + 1:\n",
    "                self.num_layers = layer\n",
    "                logging.info(\n",
    "                    f\"Stopping Layer construction: Cannot Create More Layers. Total Layers in tree: {layer}\"\n",
    "                )\n",
    "                break\n",
    "```\n",
    "\n",
    "\n",
    "3. **Cấu trúc cây cuối cùng:**\n",
    "- Số layer: 1 \n",
    "- Layer 0: 7 leaf nodes\n",
    "- Layer 1: 4 nodes tổng hợp\n",
    "\n",
    "4. **Đánh giá:**\n",
    "- Cây đã được xây dựng thành công với cấu trúc phân cấp\n",
    "- Clustering hoạt động tốt, tạo được các nhóm nodes có ý nghĩa\n",
    "- Summarization hoạt động tốt, giảm độ dài text xuống ~100 tokens\n",
    "\n",
    "Để xem chi tiết nội dung của cây, chúng ta có thể thêm hàm phân tích:\n",
    "\n",
    "```python\n",
    "def analyze_tree_content(tree):\n",
    "    print(\"=== Tree Content Analysis ===\")\n",
    "    \n",
    "    print(\"\\nLeaf Nodes:\")\n",
    "    for node in tree.leaf_nodes:\n",
    "        print(f\"\\nNode {node.index}:\")\n",
    "        print(f\"Text preview: {node.text[:100]}...\")\n",
    "        \n",
    "    print(\"\\nLayer Nodes:\")\n",
    "    for layer, nodes in tree.layer_to_nodes.items():\n",
    "        print(f\"\\nLayer {layer}:\")\n",
    "        for node in nodes:\n",
    "            if node not in tree.leaf_nodes:\n",
    "                print(f\"\\nNode {node.index}:\")\n",
    "                print(f\"Summary: {node.text[:100]}...\")\n",
    "                print(f\"Children: {node.children}\")\n",
    "\n",
    "# Analyze tree content\n",
    "analyze_tree_content(RA.tree)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e6fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "277ab6ea-1c79-4ed1-97de-1c2e39d6db2e",
   "metadata": {},
   "source": [
    "## Using other Open Source Models for Summarization/QA/Embeddings\n",
    "\n",
    "If you want to use other models such as Llama or Mistral, you can very easily define your own models and use them with RAPTOR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f86cbe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 11:09:04,261 - Loading faiss with AVX2 support.\n",
      "2024-11-15 11:09:04,280 - Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from raptor import BaseSummarizationModel, BaseQAModel, BaseEmbeddingModel, RetrievalAugmentationConfig\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa59869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets transformers torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a7cd3",
   "metadata": {},
   "source": [
    "BUG KHI CHẠY TRÊN LOCAL: \n",
    "```\n",
    "ImportError: The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywidgets` module: `pip install ipywidgets`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5cef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive - Hanoi University of Science and Technology\\GIT\\GIT2\\RAG_RAPTOR\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\User\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# # if you want to use the Gemma, you will need to authenticate with HuggingFace, Skip this step, if you have the model already downloaded\n",
    "# from huggingface_hub import login\n",
    "# login()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "245b91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# You can define your own Summarization model by extending the base Summarization Class. \n",
    "class GEMMASummarizationModel(BaseSummarizationModel):\n",
    "    def __init__(self, model_name=\"google/gemma-2b-it\"):\n",
    "        # Initialize the tokenizer and the pipeline for the GEMMA model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.summarization_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),  # Use \"cpu\" if CUDA is not available\n",
    "        )\n",
    "\n",
    "    def summarize(self, context, max_tokens=150):\n",
    "        # Format the prompt for summarization\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Write a summary of the following, including as many key details as possible: {context}:\"}\n",
    "        ]\n",
    "        \n",
    "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        # Generate the summary using the pipeline\n",
    "        outputs = self.summarization_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        \n",
    "        # Extracting and returning the generated summary\n",
    "        summary = outputs[0][\"generated_text\"].strip()\n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a171496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEMMAQAModel(BaseQAModel):\n",
    "    def __init__(self, model_name= \"google/gemma-2b-it\"):\n",
    "        # Initialize the tokenizer and the pipeline for the model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.qa_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        )\n",
    "\n",
    "    def answer_question(self, context, question):\n",
    "        # Apply the chat template for the context and question\n",
    "        messages=[\n",
    "              {\"role\": \"user\", \"content\": f\"Given Context: {context} Give the best full answer amongst the option to question {question}\"}\n",
    "        ]\n",
    "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        # Generate the answer using the pipeline\n",
    "        outputs = self.qa_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        \n",
    "        # Extracting and returning the generated answer\n",
    "        answer = outputs[0][\"generated_text\"][len(prompt):]\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878f7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "class SBertEmbeddingModel(BaseEmbeddingModel):\n",
    "    def __init__(self, model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def create_embedding(self, text):\n",
    "        return self.model.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "007f6ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from raptor import (\n",
    "    RetrievalAugmentationConfig, \n",
    "    BaseSummarizationModel,\n",
    "    BaseQAModel,\n",
    "    BaseEmbeddingModel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1630904",
   "metadata": {},
   "source": [
    "Bug: ValueError: Tokenizer class GemmaTokenizer does not exist or is not currently imported.\n",
    "```\n",
    "RAC = RetrievalAugmentationConfig(summarization_model=GEMMASummarizationModel(), qa_model=GEMMAQAModel(), embedding_model=SBertEmbeddingModel())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04574bc",
   "metadata": {},
   "source": [
    "Dựa vào các bước mà bạn đã thực hiện và gặp vấn đề khi triển khai **RAPTOR** với **các mô hình từ GEMMA** và **Hugging Face**, dưới đây là các bước cụ thể và mã sửa đổi để xây dựng lại **cấu hình cho RAPTOR** và sử dụng mô hình GEMMA một cách hiệu quả.\n",
    "\n",
    "### Phân tích vấn đề gặp phải\n",
    "1. **ImportError** với `notebook_login` từ Hugging Face:\n",
    "   - Nguyên nhân: Bạn đang cố gắng sử dụng `notebook_login()` ngoài môi trường notebook (Jupyter hoặc Colab). Điều này không cần thiết nếu bạn đã có **access token** của Hugging Face.\n",
    "\n",
    "2. **ValueError** liên quan đến tokenizer:\n",
    "   - Nguyên nhân: Mô hình **GEMMA** có thể yêu cầu cấu hình đặc biệt cho tokenizer hoặc sử dụng **custom code**.\n",
    "\n",
    "### Cách khắc phục\n",
    "- **Bỏ qua `notebook_login`** và thay vào đó, sử dụng `login()` với **access token** từ Hugging Face.\n",
    "- Đảm bảo rằng mô hình và tokenizer **được tải đúng cách**.\n",
    "- Sử dụng mô hình **SBERT** cho embedding vì nó ổn định và dễ tích hợp.\n",
    "\n",
    "Dưới đây là phiên bản mã đã được sửa đổi:\n",
    "\n",
    "### Mã cập nhật và tối ưu\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import login\n",
    "from raptor import (\n",
    "    RetrievalAugmentationConfig,\n",
    "    BaseSummarizationModel,\n",
    "    BaseQAModel,\n",
    "    BaseEmbeddingModel,\n",
    ")\n",
    "\n",
    "# Đăng nhập vào Hugging Face (yêu cầu access token)\n",
    "login(token=\"hf_your_access_token_here\")\n",
    "\n",
    "# Định nghĩa mô hình GEMMA cho summarization\n",
    "class GEMMASummarizationModel(BaseSummarizationModel):\n",
    "    def __init__(self):\n",
    "        model_name = \"google/gemma-2b\"  # Đổi tên model nếu cần\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "    def summarize(self, context, max_tokens=150):\n",
    "        prompt = f\"Summarize the following text, including key details: {context}\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return summary[len(prompt):].strip()\n",
    "\n",
    "# Định nghĩa mô hình GEMMA cho QA\n",
    "class GEMMAQAModel(BaseQAModel):\n",
    "    def __init__(self):\n",
    "        model_name = \"google/gemma-2b\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "    def answer_question(self, context, question):\n",
    "        prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return answer[len(prompt):].strip()\n",
    "\n",
    "# Định nghĩa mô hình SBERT cho embedding\n",
    "class SBertEmbeddingModel(BaseEmbeddingModel):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def create_embedding(self, text):\n",
    "        return self.model.encode(text)\n",
    "\n",
    "# Cấu hình cho RAPTOR\n",
    "config = RetrievalAugmentationConfig(\n",
    "    summarization_model=GEMMASummarizationModel(),\n",
    "    qa_model=GEMMAQAModel(),\n",
    "    embedding_model=SBertEmbeddingModel(),\n",
    "    tb_max_tokens=150,\n",
    "    tb_num_layers=3,\n",
    "    tb_threshold=0.7,\n",
    "    tb_top_k=3,\n",
    "    tr_threshold=0.6,\n",
    "    tr_top_k=3\n",
    ")\n",
    "\n",
    "# Khởi tạo RAPTOR với cấu hình\n",
    "RA = RetrievalAugmentation(config=config)\n",
    "\n",
    "# Tải và xử lý văn bản từ file\n",
    "with open('demo/sample.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Xây dựng cây\n",
    "RA.add_documents(text)\n",
    "\n",
    "# Thực hiện truy vấn\n",
    "question = \"How did Cinderella reach her happy ending?\"\n",
    "answer = RA.answer_question(question=question)\n",
    "print(\"Answer:\", answer)\n",
    "```\n",
    "\n",
    "### Giải thích thay đổi\n",
    "1. **Xóa bỏ các bước liên quan đến `notebook_login`** và chỉ sử dụng `login()` từ `huggingface_hub`.\n",
    "2. **Tối ưu hóa pipeline** cho mô hình GEMMA, đảm bảo sử dụng đúng `device_map` để tăng tốc độ.\n",
    "3. **Sử dụng SentenceTransformer** cho embedding để đảm bảo tính ổn định và khả năng mở rộng.\n",
    "\n",
    "### Các bước tiếp theo\n",
    "1. Chạy lại mã sau khi cài đặt đầy đủ các package cần thiết:\n",
    "   ```bash\n",
    "   pip install transformers torch accelerate sentence-transformers huggingface-hub\n",
    "   ```\n",
    "\n",
    "2. Đảm bảo rằng bạn đã thay thế **access token** chính xác từ Hugging Face khi gọi hàm `login()`.\n",
    "\n",
    "Nếu bạn gặp vấn đề gì thêm, hãy cho mình biết để hỗ trợ tiếp nhé!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246dbca7",
   "metadata": {},
   "source": [
    "### BUG TÙM LUM VỚI GAMMA2B: KeyError: 'gemma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08d91a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch accelerate sentence-transformers huggingface-hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5730ad4",
   "metadata": {},
   "source": [
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd8a40",
   "metadata": {},
   "source": [
    "Mô hình google/gemma-2b là mô hình \"gated\":\n",
    "\n",
    "Đây là mô hình yêu cầu quyền truy cập đặc biệt (\"gated model\"). Mặc dù bạn có quyền truy cập, nhưng mô hình này không hoàn toàn công khai và có thể yêu cầu cấu hình đặc biệt khi tải về.\n",
    "Mô hình google/gemma-2b có thể yêu cầu sử dụng trust_remote_code=True:\n",
    "\n",
    "Mô hình này sử dụng các lớp (classes) tùy chỉnh không có sẵn trong thư viện transformers. Vì vậy, khi bạn sử dụng AutoModelForCausalLM, cần phải cho phép sử dụng mã từ xa bằng cách đặt trust_remote_code=True.\n",
    "Cần phải cài đặt các phiên bản mới nhất của thư viện transformers và accelerate:\n",
    "\n",
    "Các mô hình như GEMMA yêu cầu bạn sử dụng các phiên bản mới nhất của transformers và accelerate để tận dụng các tính năng tối ưu và hỗ trợ phần cứng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e80ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers accelerate bitsandbytes torch safetensors huggingface-hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4b2059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid token passed!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Đăng nhập vào Hugging Face\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf_your_access_token_here\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-2b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Tải tokenizer và mô hình với trust_remote_code\u001b[39;00m\n",
      "File \u001b[1;32md:\\OneDrive - Hanoi University of Science and Technology\\GIT\\GIT2\\RAG_RAPTOR\\.venv\\lib\\site-packages\\huggingface_hub\\_login.py:96\u001b[0m, in \u001b[0;36mlogin\u001b[1;34m(token, add_to_git_credential, new_session, write_permission)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m add_to_git_credential:\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     92\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken will not been saved to git credential helper. Pass\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `add_to_git_credential=True` if you want to set the git\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m credential as well.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m         )\n\u001b[1;32m---> 96\u001b[0m     \u001b[43m_login\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_to_git_credential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_to_git_credential\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_permission\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_permission\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_notebook():\n\u001b[0;32m     98\u001b[0m     notebook_login(new_session\u001b[38;5;241m=\u001b[39mnew_session, write_permission\u001b[38;5;241m=\u001b[39mwrite_permission)\n",
      "File \u001b[1;32md:\\OneDrive - Hanoi University of Science and Technology\\GIT\\GIT2\\RAG_RAPTOR\\.venv\\lib\\site-packages\\huggingface_hub\\_login.py:275\u001b[0m, in \u001b[0;36m_login\u001b[1;34m(token, add_to_git_credential, write_permission)\u001b[0m\n\u001b[0;32m    273\u001b[0m permission \u001b[38;5;241m=\u001b[39m get_token_permission(token)\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m permission \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid token passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m write_permission \u001b[38;5;129;01mand\u001b[39;00m permission \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken is valid but is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread-only\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m token is required.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease provide a new token with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m correct permission.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid token passed!"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Đăng nhập vào Hugging Face\n",
    "login(token=\"hf_your_access_token_here\")\n",
    "\n",
    "model_name = \"google/gemma-2b\"\n",
    "\n",
    "try:\n",
    "    # Tải tokenizer và mô hình với trust_remote_code\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(\"Mô hình đã được tải thành công!\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi tải mô hình: {e}\")\n",
    "\n",
    "# Ví dụ sử dụng mô hình\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54711b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\User\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'gemma'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m answer[\u001b[38;5;28mlen\u001b[39m(prompt):]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Khởi tạo và thử nghiệm\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m summarization_model \u001b[38;5;241m=\u001b[39m \u001b[43mGEMMASummarizationModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m qa_model \u001b[38;5;241m=\u001b[39m GEMMAQAModel()\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Test summarization\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 20\u001b[0m, in \u001b[0;36mGEMMASummarizationModel.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     18\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-2b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Sử dụng trust_remote_code=True để tải mô hình\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Thêm dòng này\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     27\u001b[0m     model_name,\n\u001b[0;32m     28\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Thêm dòng này\u001b[39;00m\n\u001b[0;32m     29\u001b[0m )\n",
      "File \u001b[1;32md:\\OneDrive - Hanoi University of Science and Technology\\GIT\\GIT2\\RAG_RAPTOR\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:456\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    454\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 456\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    457\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    458\u001b[0m     return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    459\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    462\u001b[0m )\n\u001b[0;32m    464\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\OneDrive - Hanoi University of Science and Technology\\GIT\\GIT2\\RAG_RAPTOR\\.venv\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:957\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m--> 957\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "File \u001b[1;32md:\\OneDrive - Hanoi University of Science and Technology\\GIT\\GIT2\\RAG_RAPTOR\\.venv\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:671\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content[key]\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    672\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n\u001b[0;32m    673\u001b[0m module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gemma'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from raptor import (\n",
    "    RetrievalAugmentationConfig,\n",
    "    BaseSummarizationModel,\n",
    "    BaseQAModel,\n",
    "    BaseEmbeddingModel\n",
    ")\n",
    "\n",
    "# Đăng nhập vào Hugging Face (thay token của bạn vào đây)\n",
    "\n",
    "login(token=\"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\")\n",
    "\n",
    "# Định nghĩa mô hình GEMMA cho summarization\n",
    "class GEMMASummarizationModel(BaseSummarizationModel):\n",
    "    def __init__(self):\n",
    "        model_name = \"google/gemma-2b\"\n",
    "        # Sử dụng trust_remote_code=True để tải mô hình\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True  # Thêm dòng này\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True  # Thêm dòng này\n",
    "        )\n",
    "\n",
    "    def summarize(self, context, max_tokens=150):\n",
    "        prompt = f\"Summarize the following text, including key details: {context}\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return summary[len(prompt):].strip()\n",
    "\n",
    "# Định nghĩa mô hình GEMMA cho QA\n",
    "class GEMMAQAModel(BaseQAModel):\n",
    "    def __init__(self):\n",
    "        model_name = \"google/gemma-2b\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True  # Thêm dòng này\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True  # Thêm dòng này\n",
    "        )\n",
    "\n",
    "    def answer_question(self, context, question):\n",
    "        prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return answer[len(prompt):].strip()\n",
    "\n",
    "# Khởi tạo và thử nghiệm\n",
    "summarization_model = GEMMASummarizationModel()\n",
    "qa_model = GEMMAQAModel()\n",
    "\n",
    "# Test summarization\n",
    "context = \"Cinderella was mistreated by her stepmother and stepsisters. However, with the help of her fairy godmother, she attended the royal ball and won the prince's heart.\"\n",
    "summary = summarization_model.summarize(context)\n",
    "print(\"Summary:\", summary)\n",
    "\n",
    "# Test QA\n",
    "question = \"How did Cinderella reach her happy ending?\"\n",
    "answer = qa_model.answer_question(context, question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aefe2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "255791ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tokenizer class GemmaTokenizer does not exist or is not currently imported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m RAC \u001b[38;5;241m=\u001b[39m RetrievalAugmentationConfig(summarization_model\u001b[38;5;241m=\u001b[39m\u001b[43mGEMMASummarizationModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, qa_model\u001b[38;5;241m=\u001b[39mGEMMAQAModel(), embedding_model\u001b[38;5;241m=\u001b[39mSBertEmbeddingModel())\n",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m, in \u001b[0;36mGEMMASummarizationModel.__init__\u001b[1;34m(self, model_name)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-2b-it\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Initialize the tokenizer and the pipeline for the GEMMA model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummarization_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m     12\u001b[0m         model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mbfloat16},\n\u001b[0;32m     13\u001b[0m         device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m),  \u001b[38;5;66;03m# Use \"cpu\" if CUDA is not available\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     )\n",
      "File \u001b[1;32md:\\OneDrive - Hanoi University of Science and Technology\\GIT\\GIT2\\RAG_RAPTOR\\.venv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:688\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    686\u001b[0m         tokenizer_class \u001b[38;5;241m=\u001b[39m tokenizer_class_from_name(tokenizer_class_candidate)\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    689\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    690\u001b[0m         )\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    693\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Tokenizer class GemmaTokenizer does not exist or is not currently imported."
     ]
    }
   ],
   "source": [
    "RAC = RetrievalAugmentationConfig(summarization_model=GEMMASummarizationModel(), qa_model=GEMMAQAModel(), embedding_model=SBertEmbeddingModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee46f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RA = RetrievalAugmentation(config=RAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe05daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('demo/sample.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "RA.add_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee5847",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How did Cinderella reach her happy ending?\"\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "\n",
    "print(\"Answer: \", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
